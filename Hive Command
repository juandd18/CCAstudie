//connect to hive in Embedded Mode

//using hive cli (old way)
hive 

//using beeline
beeline -u jdbc:hive2://

//create database
create database if not exists juandavid;

use juandavid;

//hadoop commands in beeline and hive cli
dfs -ls /user/;

//create table
create  table orders (orderId int,dateOrder date,value int, state string) 
row format delimited fields terminated by ',' stored as textfile;

//insert into table append mode (IN BEELINE THE LOCAL NOT WORK)
load data inpath '/user/cca/orders/' into table orders; 

//IN HIVE ONLY IMPORTANTE
load data local inpath '/user/cca/orders/' into table orders;

//borrar tabla
drop table orders;

create external table orders (orderId int,dateOrder date,value int, state string) 
row format delimited fields terminated by ',' 
stored as textfile location '/user/cca/orders/';

//show the sintax of creation
show create table orders;

//describe the properties of the table
describe formatted orders;

//output the result in a file results and the log in the query.log (>> => overwrite)
beeline -u jdbc:hive2:// -e 'use juandavid; select * from orders limit 3;' 2>>query.log 1> results

//create partitioned table. FIRST IS partitioned by then row format
CREATE TABLE `device_statusPartitioned`(date string,mac string,value1 float,value2 float)
partitioned by (device string) row format delimited fields terminated by ',' stored as textfile;

//create partition in a table witouth data
alter table device_statusPartitioned add partition (device="computador");

//insert data into a partition
insert into device_statuspartitioned partition (device='Ronin')
select date,mac,value1,value2 from device_status where device = 'Ronin';

//delete data of a table 
truncate table device_statuspartitioned;

//insert into partition table dinamically
set hive.exec.dynamic.partition.mode=nonstrict;
insert into device_statuspartitioned partition (device)
select * from device_status;


//in case that spark sqlContext does not find hive metastore
cp  /usr/lib/hive/conf/hive-site.xml    /usr/lib/spark/conf/

//use hive with spark sql
spark-shell --master yarn 

//make reference to the database
sqlContext.sql("use juandavid")
//make select 
sqlContext.sql("select * from orders").show()

val table = sqlContext.sql("select * from orders")
table.saveAsTable("ordersSpark")

//CHANGE FORMAT OF A TABLE 

CREATE EXTERNAL TABLE customer_test(
customer_id int, customer_fname varchar(45),
customer_lname varchar(45),
customer_email varchar(45),
customer_password varchar(45),
customer_street varchar(255),
customer_city varchar(45),
customer_state varchar(45),
customer_zipcode varchar(45)
)
row format delimited fields terminated by ','
stored as textfile
location '/user/cloudera/customers/'

CREATE TABLE customerParquet
STORED AS PARQUET
AS SELECT * FROM customer_test;

//CHANGE COMPRESSION FORMAT
//first set this variables
set hive.exec.compress.output=true;
set mapreduce.output.fileoutputformat.compress=true;

//select the compression codec that you want
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;
set io.compression.codecs;

//USE A EXTERNAL UDF (LIBRARY)
ADD jar /ubicacione/nombre.jar

CREATE TEMPORARY FUNCTION nombredelafuncion AS 'nombrepaquete.nombredeclase'

//other way
CREATE FUNCTION urldetector as 'com.dataflowdeveloper.detection.URLDetector' USING JAR 'hdfs:///udf/urldetector-1.0-jar-with-dependencies.jar', JAR 'hdfs:///udf/url-detector-0.1.15.jar';



