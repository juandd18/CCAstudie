//connect to hive in Embedded Mode

//using hive cli (old way)
hive 

//using beeline
beeline -u jdbc:hive2://

//create database
create database if not exists juandavid;

use juandavid;

//hadoop commands in beeline and hive cli
dfs -ls /user/;

//create table
create  table orders (orderId int,dateOrder date,value int, state string) 
row format delimited fields terminated by ',' stored as textfile;

//insert into table append mode (IN BEELINE THE LOCAL NOT WORK)
load data inpath '/user/cca/orders/' into table orders; 

//IN HIVE ONLY IMPORTANTE
load data local inpath '/user/cca/orders/' into table orders;

//borrar tabla
drop table orders;

create external table orders (orderId int,dateOrder date,value int, state string) 
row format delimited fields terminated by ',' 
stored as textfile location '/user/cca/orders/';

//show the sintax of creation
show create table orders;

//describe the properties of the table
describe formatted orders;

//output the result in a file results and the log in the query.log (>> => overwrite)
beeline -u jdbc:hive2:// -e 'use juandavid; select * from orders limit 3;' 2>>query.log 1> results

//create partitioned table. FIRST IS partitioned by then row format
CREATE TABLE `device_statusPartitioned`(date string,mac string,value1 float,value2 float)
partitioned by (device string) row format delimited fields terminated by ',' stored as textfile;

//create partition in a table witouth data
alter table device_statusPartitioned add partition (device="computador");

//insert data into a partition
insert into device_statuspartitioned partition (device='Ronin')
select date,mac,value1,value2 from device_status where device = 'Ronin';

//delete data of a table 
truncate table device_statuspartitioned;

//insert into partition table dinamically
set hive.exec.dynamic.partition.mode=nonstrict;
insert into device_statuspartitioned partition (device)
select * from device_status;


//in case that spark sqlContext does not find hive metastore
cp  /usr/lib/hive/conf/hive-site.xml    /usr/lib/spark/conf/

//use hive with spark sql
spark-shell --master yarn 

//make reference to the database
sqlContext.sql("use juandavid")
//make select 
sqlContext.sql("select * from orders").show()

val table = sqlContext.sql("select * from orders")
table.saveAsTable("ordersSpark")

//CHANGE FORMAT OF A TABLE 

CREATE EXTERNAL TABLE customer_test(
customer_id int, customer_fname varchar(45),
customer_lname varchar(45),
customer_email varchar(45),
customer_password varchar(45),
customer_street varchar(255),
customer_city varchar(45),
customer_state varchar(45),
customer_zipcode varchar(45)
)
row format delimited fields terminated by ','
stored as textfile
location '/user/cloudera/customers/'

CREATE TABLE customerParquet
STORED AS PARQUET
AS SELECT * FROM customer_test;

//CHANGE COMPRESSION FORMAT
//first set this variables
set hive.exec.compress.output=true;
set mapreduce.output.fileoutputformat.compress=true;

//select the compression codec that you want
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;
set io.compression.codecs;

//USE A EXTERNAL UDF (LIBRARY)
ADD jar /ubicacione/nombre.jar

CREATE TEMPORARY FUNCTION nombredelafuncion AS 'nombrepaquete.nombredeclase'

//other way
CREATE FUNCTION urldetector as 'com.dataflowdeveloper.detection.URLDetector' USING JAR 'hdfs:///udf/urldetector-1.0-jar-with-dependencies.jar', JAR 'hdfs:///udf/url-detector-0.1.15.jar';

//CHANGE THE TYPE OF A FIELD
ALTER TABLE table_name CHANGE old_col_name new_col_name new_data_type
ALTER TABLE customer_test CHANGE customer_email customer_email varchar(70)

//PURGE BAD RECORDS FROM A DATA SET
SELECT country, CASE WHEN LENGTH(os(agent)) > 0 THEN os(agent) ELSE 'Others' END AS SO, COUNT(*) 
FROM clicks_data 
WHERE country IS NOT NULL AND os(agent) IS NOT NULL 
GROUP BY country, os(agent);

//SERDES

add jar /home/cloudera/hive-json-serde-0.2.jar

CREATE EXTERNAL TABLE retail.jsonSerde (
id bigint,
created_at string,
text string,
user_id bigint,
user_name string
)
ROW FORMAT SERDE "org.apache.hadoop.hive.contrib.serde2.JsonSerde"
WITH SERDEPROPERTIES(
"id"="$.id",
"created_at"="$.created_at",
"text"="$.text",
"user_id"="$.user_id",
"user_name"="$.user_name"
)
LOCATION '/user/ccp/hive/jsontable'

ADD JAR /home/cloudera/hivexmlserde-1.0.5.3.jar

CREATE EXTERNAL TABLE retail.xmlTable(
customer_id string,
income bigint,
demographics map<string,string>,
financial map<string,string>
)
ROW FORMAT SERDE "com.ibm.spss.hive.serde2.xml.XmlSerDe"
WITH SERDEPROPERTIES(
"column.xpath.customer_id"="/record/@customer_id",
"column.xpath.income"="/record/income/text()",
"column.xpath.demographics"="/record/demographics/*",
"column.xpath.financial"="/record/financial/*"
)
STORED AS
INPUTFORMAT 'com.ibm.spss.hive.serde2.xml.XmlInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
LOCATION '/user/ccp/hive/xmldata'
TBLPROPERTIES (
"xmlinput.start"="<record customer",
"xmlinput.end"="</record>"
);

//read in spark json

val filejson = sqlContext.read.json("/user/ccp/hive/jsonfile.json")

val jsonRDD = sc.wholeTextFiles("/user/ccp/hive/jsonfile.txt").map(x => x._2)

import org.apache.spark.sql.types._

val schemad = StructType( List(
  StructField("_id",StructType(List(StructField("$oid", StringType, nullable = true)) )),
  StructField("product_name", StringType, nullable = true),
  StructField("supplier", StringType, nullable = true),
  StructField("quantity", DoubleType, nullable = true),
  StructField("unit_cost", StringType, nullable = true)
))


val filejson = sqlContext.read.schema(schemad).json(jsonRDD).toDF()
filejson.show()
