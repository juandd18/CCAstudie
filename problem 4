

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera
--table orders --fields-terminated-by '\t' --lines-terminated-by '\n' --target-dir /user/cloudera/problem5/text --as-textfile;

//Import orders table from mysql  into hdfs to the destination /user/cloudera/problem5/avro. File should be stored as avro file.
sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera 
--target-dir /user/cloudera/problem5/avro --table orders --as-avrodatafile -m 1;

//Import orders table from mysql  into hdfs  to folders /user/cloudera/problem5/parquet. File should be stored as parquet file.
sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --table orders
--target-dir user/cloudera/problem5/parquet --as-parquetfile;


Transform/Convert data-files at /user/cloudera/problem5/avro and store the converted file at the following locations and file formats
save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem5/parquet-snappy-compress
save the data to hdfs using gzip compression as text file at /user/cloudera/problem5/text-gzip-compress

import com.databricks.spark.avro._

val orders = sqlContext.read.avro("/user/cloudera/problem5/avro")
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")
orders.write.parquet("/user/cloudera/problem5/parquet-snappy-compress")

orders.repartition(1).write.parquet("/user/cloudera/problem5/parquet-snappy-compress");

orders.map(x=> x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)).saveAsTextFile("/user/cloudera/problem5/text-gzip-compress",
classOf[org.apache.hadoop.io.compress.GzipCodec])

orders.map(x=> (x(0).toString,x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3))).saveAsSequenceFile("/user/cloudera/problem5/sequence")

//Below may fail in some cloudera VMS. If the spark command fails use the sqoop command to accomplish the problem. 
orders.map(x=> x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)).saveAsTextFile("/user/cloudera/problem5/text-gzip-compress2",
classOf[org.apache.hadoop.io.compress.SnappyCodec])

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera 
--target-dir /user/cloudera/problem5/text-snappy-compress --table orders --compress 
--compression-codec org.apache.hadoop.io.compress.SnappyCodec --as-textfile -m 1;

Transform/Convert data-files at /user/cloudera/problem5/parquet-snappy-compress and store the converted file at the following locations and file formats
save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress
save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy

val orders = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
orders.write.parquet("/user/cloudera/problem5/parquet-no-compress")

import com.databricks.spark.avro._
val orders = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
orders.write.avro("/user/cloudera/problem5/avro-snappy")

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera 
--target-dir /user/cloudera/problem5/avro-snappy --table orders --compress 
--compression-codec org.apache.hadoop.io.compress.SnappyCodec --as-avrodatafile ;


Transform/Convert data-files at /user/cloudera/problem5/avro-snappy and store the converted file at the following locations and file formats
save the data to hdfs using no compression as json file at /user/cloudera/problem5/json-no-compress
save the data to hdfs using gzip compression as json file at /user/cloudera/problem5/json-gzip

import com.databricks.spark.avro._
val orders = sqlContext.read.avro("/user/cloudera/problem5/avro-snappy")
sqlContext.setConf("spark.sql.compression.codec","uncompressed")
orders.write.json("/user/cloudera/problem5/json-no-compress")

sqlContext.setConf("spark.sql.compression.codec","gzip")
orders.write.json("/user/cloudera/problem5/json-gzip")

orders.toJSON.saveAsTextFile("/user/cloudera/problem5/json-no-compress");
orders.toJSON.saveAsTextFile("/user/cloudera/problem5/json-gzip",classOf[org.apache.hadoop.io.GzipCodec]);

Transform/Convert data-files at  /user/cloudera/problem5/json-gzip and store the converted file at the following locations and file formats
save the data to as comma separated text using gzip compression at   /user/cloudera/problem5/csv-gzip

val orders = sqlContext.read.json("/user/cloudera/problem5/json-gzip")
orders.map(x=> x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/cloudera/problem5/csv-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])


Using spark access data at /user/cloudera/problem5/sequence and stored it back to hdfs using no compression as ORC file 
to HDFS to destination /user/cloudera/problem5/orc 

val orders = sc.sequenceFile("/user/cloudera/problem5/sequence")

//To read the sequence file you need to understand the sequence getter for the key and value class to //be used while loading the sequence file as a spark RDD.
//In a new terminal Get the Sequence file to local file system
hadoop fs -get /user/cloudera/problem5/sequence/part-00000
//read the first 300 characters to understand the two classes to be used. 
cut -c-300 part-00000

//In spark shell do below
var seqData = sc.sequenceFile("/user/cloudera/problem5/sequence/",classOf[org.apache.hadoop.io.Text],classOf[org.apache.hadoop.io.Text]);
seqData.map(x=>{var d = x._2.toString.split("\t"); (d(0),d(1),d(2),d(3))}).toDF().write.orc("/user/cloudera/problem5/orc");








