

1.
sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db"
 --username root --password cloudera --table orders --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --target-dir /user/cloudera/problem1/orders --as-avrodatafile;

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --table order_items --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --target-dir /user/cloudera/problem2/orders_items --as-avrodatafile;

3.

import com.databricks.spark.avro._

val orders =  sqlContext.read.avro("/user/cloudera/problem1/orders")
val orders_items = sqlContext.read.avro("/user/cloudera/problem2/orders_items")

val join_order = orders.as("ord").join(orders_items.as("it"),$"ord.order_id" === $"it.order_item_order_id")

import org.apache.spark.sql.functions._;

val result_agg = join_order.groupBy(to_date(from_unixtime(col("order_date")/1000)).alias("order_formatted_date"),$"order_status")
.agg(sum("order_item_subtotal").alias("total_amount"),count("order_item_id").alias("total_orders"))
.orderBy($"order_formatted_date".desc,$"order_status".asc,$"total_amount".desc,$"total_orders".asc)

sqlContext.setConf("spark.sql.parquet.compression.codec","gzip");
result_agg.write.parquet("/user/cloudera/problem1/result4a-gzip");

sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");
result_agg.write.parquet("/user/cloudera/problem1/result4a-snappy");

sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" 
--username root --password cloudera --table result 
--export-dir /user/cloudera/problem1/result4a-csv --fields-terminated-by ',';

sqoop export \
--table result \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--export-dir "/user/cloudera/problem1/result4a-csv" \
--columns "order_date,order_status,total_amount,total_orders"
