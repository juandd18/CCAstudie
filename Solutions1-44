//ejercisio 1 

create database PatientInfo location '/user/hive/warehouse/patient';

create table patientinfo.patient
(
name struct<name: string, lastname: string>,
address struct<houseno: string,localityname: string,city: string,zip: string>,
phone struct<mobile: string, landline: string>
)
STORED AS SEQUENCEFILE;

create table patientinfo.temppatient
(
name string,
lastname string,
data string,
mobile string,
landline string,
zip string
)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '|'
STORED AS TEXTFILE

LOAD DATA LOCAL INPATH '/home/cloudera/problem1data.txt' INTO TABLE patientinfo.temppatient

INSERT INTO patientinfo.patient
SELECT named_struct('name',name,'lastname',lastname), named_struct('houseno',split(data,'\\,')[0],'localityname',split(data,'\\,')[1],'city',split(data,'\\,')[2],'zip',zip), named_struct('mobile',mobile,'landline',landline) from patientinfo.temppatient

//ejercisio 2

64.242.88.10 - - [07/Jun/2016:16:47:46 -0800] "GET /hadoopexam.com/bin/rdiff/Know/ReadmeFirst?rev1=1.5?rev2=1.4 HTTP/1.1" 200 5724
64.242.88.10 - - [07/Jun/2016:16:49:04 -0800] "GET /hadoopexam.com/bin/view/Main/hadoopexam.comGroups?rev=1.2 HTTP/1.1" 200 5162
64.242.88.10 - - [07/Jun/2016:16:50:54 -0800] "GET /hadoopexam.com/bin/rdiff/Main/ConfigurationVariables HTTP/1.1" 200 59679
64.242.88.10 - - [07/Jun/2016:16:52:35 -0800] "GET /hadoopexam.com/bin/edit/Main/Flush_service_name?topicparent=Main.ConfigurationVariables HTTP/1.1" 401 12851
64.242.88.10 - - [07/Jun/2016:16:53:46 -0800] "GET /hadoopexam.com/bin/rdiff/hadoopexam.com/hadoopexam.comRegistration HTTP/1.1" 200 34395
64.242.88.10 - - [07/Jun/2016:16:54:55 -0800] "GET /hadoopexam.com/bin/rdiff/Main/NicholasLee HTTP/1.1" 200 7235
64.242.88.10 - - [07/Jun/2016:16:56:39 -0800] "GET /hadoopexam.com/bin/view/Sandbox/WebHome?rev=1.6 HTTP/1.1" 200 8545
64.242.88.10 hadoopexam_user hdpexam [07/Jun/2016:16:58:54 -0800] "GET /hadoopexam.com/listinfo/administration HTTP/1.1" 200 6459

hdfs dfs -put problem2data.txt /user/ccp/


CREATE TABLE patientinfo.logdata
(
host string,
identity string,
user string,
time string,
request string,
status string,
size string
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
'input.regex'='^(\\S+) (\\S+) (\\S+) \\[([^\\[]*)\\] "(.+)" (\\S+) (\\S+)',
'output.format.string'='%1$s %2$s %3$s %4$s %5$s %6$s %7$s'
)

LOAD DATA INPATH '/user/ccp/problem2data.txt' INTO TABLE patientinfo.logdata

//ejercisio 3

July  3 07:10:54 hadoopexam.com_node01 init: tty (/dev/tty6) main process (1208) killed by TERM signal
July  3 07:11:31 hadoopexam.com_node01 kernel: registered taskstats version 1
July  3 07:11:31 hadoopexam.com_node01 kernel: sr0: scsi3-mmc drive: 32x/32x xa/form2 tray
July  3 07:11:31 hadoopexam.com_node01 kernel: piix4_smbus 0000:00:07.0: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
July  3 07:11:31 hadoopexam.com_node01 kernel: nf_conntrack version 0.5.0 (7972 buckets, 31888 max)
July  3 07:11:57 hadoopexam.com_node01 kernel: hrtimer: interrupt took 11250457 ns
July  3 07:11:59 hadoopexam.com_node01 ntpd_initres[1705]: host name not found: 0.rhel.pool.ntp.org

^(\\S+)\s*(\\S+) (\\S+) (\\S+) (\\S+): (.+)   

log string following format:

month=jun
day=3
time=07:10:54
node=hadoopexam.com_node01
process=init
log_meg = the rest of the log file

CREATE EXTERNAL TABLE patientinfo.logproblem3
(
day string,
time string,
node string,
process string,
log_meg string
)
PARTITIONED BY (year int, month int)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES(
'input.regex'='^(\\S+)\s*(\\S+) (\\S+) (\\S+) (\\S+): (.+)'
)
LOCATION '/user/ccp/problem3';

ALTER TABLE patientinfo.logproblem3 ADD IF NOT EXISTS PARTITION(year=2017,month=6)
LOCATION '/user/ccp/problem3_1.log'

ALTER TABLE patientinfo.logproblem3 ADD IF NOT EXISTS PARTITION(year=2017,month=6)
LOCATION '/user/ccp/problem3_2.log'

//ejercicio 4

CREATE TABLE patientinfo.tmpproblem5
(
name string,
lname string,
empid string,
loggedindate string,
joiningdate string,
deptid string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES('skip.header.line.count'='1');

LOAD DATA LOCAL INPATH '/home/cloudera/problem5data.txt' INTO TABLE patientinfo.tmpproblem5;

ALTER TABLE patientinfo.tmpproblem5 CHANGE loggedindate loggedindate BIGINT;

CREATE TABLE patientinfo.problem5 AS
select name,lname,empid,MAX(loggedindate),joiningdate,deptid from tmpproblem5 GROUP BY name,lname,empid,joiningdate,deptid


//ejercisio 5

CREATE TABLE patientinfo.problem6Patient
(
id int,
name string,
lname string,
age int,
birthdate string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES(
'skip.header.line.count'='1'
);

LOAD DATA LOCAL INPATH '/home/cloudera/problem6patient.txt' INTO TABLE patientinfo.problem6Patient;

CREATE TABLE patientinfo.problem5healt
(
id int,
lowbp int,
highbp int,
ldl int,
totalcol int,
triglycerides int
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES(
'skip.header.line.count'='1'
);

LOAD DATA INPATH '/user/ccp/problem6health.txt' INTO TABLE patientinfo.problem5healt;

CREATE TABLE patientinfo.mergeproblem6 as
SELECT p.id,p.name,p.lname,p.age,p.birthdate,h.lowbp,h.highbp,h.ldl,h.totalcol,h.triglycerides from problem6patient p left outer join problem5healt h on p.id = h.id

different posibles formats and his regex in order to use it with case
--'dd-MMM-yyyy' (\d{1,2})\-(\S{3})\-(\d{4})
--'mm/dd/yyyy' (\S{2})\/(\d{1,2})\/(\d{4})
--'dd/mmm/yyyy' (\d{1,2})\/(\S{3})\/(\d{4})
-- 'mmm-dd yyyy' (\S{3})\-(\d{1,2})\s+(\d{4})

SELECT SELECT p.id,p.name,p.lname,p.age, 
CASE 
WHEN p.birthdate REGEXP '(\\d{1,2})\\-(\\S{3})\\-(\\d{4})' THEN FROM_UNIXTIME(UNIX_TIMESTAMP(p.birthdate,'dd-MMM-yyyy'),'MM/dd/yyyy')
WHEN p.birthdate REGEXP '(\\S{2})\\/(\\d{1,2})\\/(\d{4})' THEN FROM_UNIXTIME(UNIX_TIMESTAMP(p.birthdate,'mm/dd/yyyy'),'MM/dd/yyyy')
WHEN p.birthdate REGEXP '(\\d{1,2})\\-(\\S{3})\\-(\\d{4})' THEN FROM_UNIXTIME(UNIX_TIMESTAMP(p.birthdate,'dd/MMM/yyyy'),'MM/dd/yyyy')
WHEN p.birthdate REGEXP '(\\S{3})\\-(\\d{1,2})\\s+(\\d{4})' THEN FROM_UNIXTIME(UNIX_TIMESTAMP(p.birthdate,'MMM-dd yyyy'),'MM/dd/yyyy')
ELSE
p.birthdate
END,
h.lowbp,h.highbp,h.ldl,h.totalcol,h.triglycerides from problem6patient p left outer join problem5healt h on p.id = h.id

----transform date or get dates
FROM_UNIXTIME(UNIX_TIMESTAMP(p.birthdate,'dd-MMM-yyyy'),'MM/dd/yyyy')

//ejercicio 6

July  3 07:11:31 hadoopexam.com_node01 kernel: registered taskstats version 1

(\\S+)\\s+(\\S+)\\s+(\\S+)\\s+(\\S+)\\s+(\\S+):\\s+(.*)

CREATE TABLE patientinfo.tempproblem6log
(
month string,
day string,
time string,
host string,
proc string,
message string
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES(
'input.regex'='(\\S+)\\s+(\\S+)\\s+(\\S+)\\s+(\\S+)\\s+(\\S+):\\s+(.*)'
)

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem6_log1.txt' INTO TABLE patientinfo.tempproblem6log

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem6_log2.txt' OVERWRITE INTO TABLE patientinfo.tempproblem6log


CREATE TABLE patientinfo.problem6log
(
time string,
host string,
proc string,
message string
)
PARTITIONED BY (year int,mes string,dia string)

INSERT OVERWRITE TABLE patientinfo.problem6log PARTITION (year=2016,mes,dia)
select time,host,proc,message,month as mes,day as dia from patientinfo.tempproblem6log

//ejercicio 7
1,http://hadoopexam.com/path1/p.php?keyword=hadoop&country=india#Ref1,30/JUN/2016,PUBHADOOPEXAM,GOOGLEADSENSE

keyword=(\S*)&
http:\/+(\w*\.\w*)
country=(\S*)\#

CREATE TABLE patientinfo.tempproblem7
(
ID string,
URL string,
DATE string,
PUBID string,
ADVERTISERID string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','

LOAD DATA INPATH '/user/ccp/problem7/problem7log.txt' INTO TABLE patientinfo.tempproblem7 

CREATE TABLE patientinfo.problem7
(
id string,
date string,
public string,
advertiserid string,
keyword string
)
PARTITIONED BY (host string, country string)

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;

INSERT INTO TABLE patientinfo.problem7 PARTITION (host,country)
SELECT id,date,pubid,advertiserid, regexp_extract(url,'keyword=(\\S*)&') as keyword, regexp_extract(url,'http:\\/+(\\w*\\.\\w*)') as host,
regexp_extract(url,'country=(\\S*)\\#') as country FROM tempproblem7

//ejercicio 8

ID,URL,DATE,PUBID,ADVERTISERID
1,http://hadoopexam.com/path1/p.php?keyword=hadoop&country=india#Ref1,30/JUN/2016,PUBHADOOPEXAM,GOOGLEADSENSE

http:\/+(\w*\.\w*)
country=(\w*)\#

CREATE TABLE patientinfo.tmpproblem8log
(
id int,
url string,
date string,
pubid string,
advertiserid string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES(
'skip.header.line.count'='1'
)

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem8log.txt' INTO TABLE patientinfo.tmpproblem8log

--utilizar parse_url function

CREATE TABLE patientinfo.problem8log AS
select distinct id,date,pubid,advertiserid, 
regexp_extract(url,'http:\\/+(\\w*\\.\\w*)') as domain, 
regexp_extract(url,'country=(\\w*)\\#') as country from tmpproblem8log

CREATE TABLE patientinfo.problem8ads
(
id int,
adname string, 
cpc int
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES(
'skip.header.line.count'='1'
)

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem8ads.txt' INTO TABLE patientinfo.problem8ads

CREATE TABLE patientinfo.mergeproblem8 AS
SELECT l.id,l.date,l.pubid,l.advertiserid,l.domain,l.country,a.adname,a.cpc FROM problem8log l inner join problem8ads a on a.id = l.id 

SELECT adname,sum(cpc) as sumExpenses from mergeproblem8 group by domain,country,adname order by sumExpenses desc

//ejercicio 9

7,http://hadoopexam.com/path1/p.php?keyword=jee&country=india#Ref1,01/JUL/2016,PUBHADOOPEXAM,GOOGLEADSENSE

keyword=(\S*)\&
country=(\S*)\#
http:\/+(\w*\.\w*)

CREATE TABLE patientinfo.tempproblem9
(
ID int,
URL string,
DATE string,
PUBID string,
ADVERTISERID string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES(
'skip.header.line.count'='1'
)

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem9_1.txt' INTO TABLE patientinfo.tempproblem9

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem9_2.txt' INTO TABLE patientinfo.tempproblem9

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem9_3.txt' INTO TABLE patientinfo.tempproblem9

id int,
date string
pubid string
advertiserid string
keyword string
host string 
country string

CREATE TABLE patientinfo.problem9
(
id int,
date string,
pubid string,
advertiserid string,
keyword string,
host string ,
country string
)


INSERT INTO patientinfo.problem9 
SELECT distinct id,date,lower(pubid) ,lower(ADVERTISERID),
regexp_extract(url,'keyword=(\\S*)\\&') ,
regexp_extract(url,'http:\/+(\\w*\\.\\w*)')  ,
regexp_extract(url,'country=(\\S*)\\#') from tempproblem9
WHERE regexp_extract(url,'http:\/+(\\w*\\.\\w*)') <> ''

//ejercicio 10

CREATE TABLE patientinfo.problem10
(
Id int,
Name string,
LowBP int,
HighBP int,
LDL int,
TotalCol int,
Triglycerides int
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES(
'skip.header.line.count'='1'
)

LOAD DATA LOCAL INPATH '/home/cloudera/ccp/problem10.txt' INTO TABLE patientinfo.problem10

ADD JAR /usr/lib/hive/lib/hive-exec.jar

CREATE TEMPORARY FUNCTION exfunction AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower';

select *,exfunction(name) from patientinfo.problem10

//ejercicio 11

create table employee(id int,name varchar(25),sex varchar(10),age int)

create table salary(id int,salary bigint);

create table patientinfo.tmpemployee
(
id int,
name string,
sex string,
age int
)
row format delimited
fields terminated by ','


create table patientinfo.tmpsalary
(
id int,
salary bigint
)
row format delimited 
fields terminated by ','


load data local inpath '/home/cloudera/ccp/problem11Empleyee.txt' into table patientinfo.tmpemployee

load data local inpath '/home/cloudera/ccp/problem11Salary.txt' into table patientinfo.tmpsalary

create table patientinfo.employee
stored as avro as
select * from tmpemployee

create table patientinfo.salary
stored as avro as
select * from tmpsalary

sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --hcatalog-database patientinfo --hcatalog-table employee --table employee --columns "id,name,sex,age"
 
sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --hcatalog-database patientinfo --hcatalog-table salary --table salary --columns "id,salary"

//ejercicio 13

hdfs dfs -getmerge -help

hdfs dfs -mkidr /user/ccp/foldercreation
hdfs dfs -touchz /user/ccp/foldercreation/file

hdfs dfs -put problem131.txt /user/ccp/foldercreation
hdfs dfs -put problem132.txt /user/ccp/foldercreation

hdfs dfs -getmerge  /user/ccp/foldercreation/ /home/cloudera/transfiles/recordsskip.txt

hdfs dfs -put /home/cloudera/transfiles/* /user/ccp/folder

//ejercicio 16

CREATE DATABASE datapropertiestwo 
LOCATION '/user/ccp/dataproperties'
WITH DBPROPERTIES(
'data.file.location'='/user/ccp/dataproperties',
'database.creator'='juan',
'database.created.on'='jul2017'
)

describe database extended datapropertiestwo;

//ejercicio 18

CREATE TABLE patientinfo.problem18
(
name string,
bussinessplace array<string>,
sexage struct<sex:string,age:int>,
fathernamechild map<string,string>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY ':'
MAP KEYS TERMINATED BY ':' ;

//ejercicio 20

sqoop eval --query "show tables;" --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera
--falta mirar si tengo permisos para leer base de datos

//ejercicio 23
sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera 
--query "select * from order_items oi inner join orders o on oi.order_item_order_id = o.order_id where \$CONDITIONS" 
--target-dir /user/ccp/problem23 --as-avrodatafile -m 1 --split-by order_id

// ejercicio 24
--every time change the value '--last-value'
sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera 
--table departments --target-dir /user/ccp/departments --fields-terminated-by '|' 
--check-column department_id --incremental append --last-value 7

//ejercicio 25 

CREATE DATABASE problem25;

CREATE TABLE problem25.categories
(
category_id int,
category_deparment_id int,
category_name string
)
stored as avro

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera 
--table categories --hive-home '/user/hive/warehouse' --hive-import 
--hive-database problem25 --hive-overwrite --hive-table categories 

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root 
--password cloudera --table categories  
--create-hive-table --hive-home '/user/hive/warehouse' --hive-import --hive-database problem25 

//ejercicio 27
CREATE TABLE patientinfo.problem27
(
department_id int,
department_name string
)


sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --table departments --hive-import --hive-home /user/hive/warehouse --hive-table patientinfo.problem27

sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --table departments --hive-import --hive-home /user/hive/warehouse --hive-table patientinfo.problem27 --check-column department_id --incremental append --last-value 8

//ejercicio 29

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --table update_departments --hive-import --hive-home /user/hive/warehouse --hive-table patientinfo.problem29 --create-hive-table

sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username root --password cloudera --table update_departments --hive-import --hive-home /user/hive/warehouse --hive-table patientinfo.problem29 --incremental lastmodified --check-column department_id --last-value 1000

//ejercicio 37

# the components on this agent
a1.sources = source1
a1.sinks = sink1
a1.channels = channel1

# Describe/configure the source
a1.sources.source1.type = exec
a1.sources.source1.command = tail -F /opt/gen_logs/logs/access.log

# Describe the sink
a1.sinks.sink1.type = hdfs
a1.sinks.sink1.hdfs.path = /user/ccp/flume/problem37

# Use a channel which buffers events in memory
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 1000
a1.channels.channel1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.source1.channels = channel1
a1.sinks.sink1.channel = channel1

bin/flume-ng agent --conf . --conf-file flume37.conf --name a1 -Dflume.root.logger=INFO,console


//ejercicio 38 

CREATE EXTERNAL TABLE patientinfo.personsalary
(
name string,
salary bigint,
sex string,
age int
)
row format delimited
fields terminated by ','
LOCATION '/user/hive/warehouse/patientinfo.db/personsalary'

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = source1
a1.sinks = sink1
a1.channels = channel1

# Describe/configure the source
a1.sources.source1.type = netcat
a1.sources.source1.bind = localhost
a1.sources.source1.port = 44444

# Describe the sink
a1.sinks.sink1.type = hdfs
a1.sinks.sink1.hdfs.path = /user/hive/warehouse/patientinfo.db/personsalary
a1.sinks.sink1.hdfs.fileType = DataStream
a1.sinks.sink1.hdfs.writeFormat = Text
a1.sinks.sink1.hdfs.filePrefix = test-events

# Use a channel which buffers events in memory
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 1000
a1.channels.channel1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.source1.channels = channel1
a1.sinks.sink1.channel = channel1

flume-ng agent --conf . --conf-file flumeproblem38.conf --name a1 -Dflume.root.logger=INFO,console --classpath "/usr/lib/hive-hcatalog/share/hcatalog/*":"/usr/lib/hive/lib/*"

//ejercicio 39

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = source1
a1.sinks = sink1
a1.channels = channel1

# Describe/configure the source
a1.sources.source1.type = exec
a1.sources.source1.command = tail -F /opt/gen_logs/logs/access.log
a1.sources.source1.interceptors = i1
a1.sources.source1.interceptors.i1.type = timestamp

# Describe the sink
a1.sinks.sink1.type = hdfs
a1.sinks.sink1.hdfs.path = /user/ccp/flume/problem39/flume-%Y-%m-%d%H%M

# Use a channel which buffers events in memory
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 1000
a1.channels.channel1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.source1.channels = channel1
a1.sinks.sink1.channel = channel1

flume-ng agent --conf . --conf-file flumeproblem39.conf --name a1 -Dflume.root.logger=INFO,console 







//ejercicio 40

flume-ng agent --conf . --conf-file flumeproblem40.conf --name a1 -Dflume.root.logger=INFO,console 

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = source1
a1.sinks = sink1
a1.channels = channel1

# Describe/configure the source
a1.sources.source1.type = netcat
a1.sources.source1.bind = localhost
a1.sources.source1.port = 44444
a1.sources.source1.interceptors = i1
a1.sources.source1.interceptors.i1.type = regex_filter
a1.sources.source1.interceptors.i1.regex = male

# Describe the sink
a1.sinks.sink1.type = hdfs
a1.sinks.sink1.hdfs.path = /user/hive/warehouse/patientinfo.db/personsalary
a1.sinks.sink1.hdfs.fileType = DataStream
a1.sinks.sink1.hdfs.writeFormat = Text

# Use a channel which buffers events in memory
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 1000
a1.channels.channel1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.source1.channels = channel1
a1.sinks.sink1.channel = channel1

nc localhost 44444










//ejercicio 41 


CREATE EXTERNAL TABLE patientinfo.maleinfo
(
sex string,
name string,
city string
)
row format delimited
fields terminated by ','
location '/user/hive/warehouse/patientinfo.db/maleinfo'

CREATE EXTERNAL TABLE patientinfo.femaleinfo
(
sex string,
name string,
city string
)
row format delimited
fields terminated by ','
location '/user/hive/warehouse/patientinfo.db/femaleinfo'

flume-ng agent --conf . --conf-file flumeproblem41.conf --name a1 -Dflume.root.logger=INFO,console 



# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = source1 
a1.sinks = sink1 sink2
a1.channels = channel1 channel2

# Describe/configure the source 1
a1.sources.source1.type = exec
a1.sources.source1.command = tail -F /home/cloudera/ccp/flume/dataproblem41.txt
a1.sources.source1.interceptors = i1
a1.sources.source1.interceptors.i1.type = regex_extractor
a1.sources.source1.interceptors.i1.regex = ^male|female
a1.sources.source1.interceptors.i1.serializer = t1
a1.sources.source1.interceptors.i1.serializer.t1.name = multi

a1.sources.source1.selector.type = multiplexing
a1.sources.source1.selector.header = multi
a1.sources.source1.selector.mapping.male = channel1
a1.sources.source1.selector.mapping.female = channel2

# Describe the sink
a1.sinks.sink1.type = hdfs
a1.sinks.sink1.hdfs.path = /user/hive/warehouse/patientinfo.db/maleinfo
a1.sinks.sink1.hdfs.fileType = DataStream
a1.sinks.sink1.hdfs.batchSize = 1
a1.sinks.sink1.hdfs.rollInterval = 0
a1.sinks.sink1.hdfs.writeFormat = Text

# Describe the sink 2
a1.sinks.sink2.type = hdfs
a1.sinks.sink2.hdfs.path = /user/hive/warehouse/patientinfo.db/femaleinfo
a1.sinks.sink2.hdfs.fileType = DataStream
a1.sinks.sink2.hdfs.batchSize = 1
a1.sinks.sink2.hdfs.rollInterval = 0
a1.sinks.sink2.hdfs.writeFormat = Text

# Use a channel which buffers events in memory
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 1000
a1.channels.channel1.transactionCapacity = 100

a1.channels.channel2.type = memory
a1.channels.channel2.capacity = 1000
a1.channels.channel2.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.source1.channels = channel1 channel2
a1.sinks.sink1.channel = channel1
a1.sinks.sink2.channel = channel2


//ejercicio 42 

flume-ng agent --conf . --conf-file flumeproblem42.conf --name a1 -Dflume.root.logger=INFO,console

#NO REPETIR ARCHIVO EN EL FOLDER
echo "soy juandavid" > /home/cloudera/ccp/flume/problem42/file.txt

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = source1
a1.sinks = sink1
a1.channels = channel1

# Describe/configure the source
a1.sources.source1.type = spooldir
a1.sources.source1.spoolDir = /home/cloudera/ccp/flume/problem42

# Describe the sink
a1.sinks.sink1.type = hdfs
a1.sinks.sink1.hdfs.path = /user/ccp/flume/problem42
a1.sinks.sink1.hdfs.filePrefix = events
a1.sinks.sink1.hdfs.fileSuffix = .log
a1.sinks.sink1.hdfs.inUseFilePrefix = _
a1.sinks.sink1.hdfs.fileType = DataStream
a1.sinks.sink1.hdfs.writeFormat = Text

# Use a channel which buffers events in memory
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 1000
a1.channels.channel1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.source1.channels = channel1
a1.sinks.sink1.channel = channel1

//ejercicio 43

flume-ng agent --conf . --conf-file flumeproblem43.conf --name a1 -Dflume.root.logger=INFO,console


echo "soy juandavid" > /home/cloudera/ccp/flume/problem43/file.txt

echo "soy juandavid" > /home/cloudera/ccp/flume/problem43_1/file.tx


# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1 r2
a1.sinks = k1
a1.channels = c1

# Describe/configure the source r1
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/cloudera/ccp/flume/problem43

# Describe/configure the source r2
a1.sources.r2.type = spooldir
a1.sources.r2.spoolDir = /home/cloudera/ccp/flume/problem43_1

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /user/ccp/flume/problem43
a1.sinks.k1.hdfs.filePrefix = events
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.inUsePrefix = _

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sources.r2.channels = c1
a1.sinks.k1.channel = c1

//Ejercicio 44

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2 

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/cloudera/ccp/flume/problem44
a1.sources.r1.selector.type = replicating
a1.sources.r1.selector.optional = c2

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.dir = /user/ccp/flume/problem44
a1.sinks.k1.hdfs.filePrefix = event
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.inUsePrefix = _

# Describe the sink
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.dir = /user/ccp/flume/problem44_1
a1.sinks.k2.hdfs.filePrefix = event
a1.sinks.k2.hdfs.fileSuffix = .log
a1.sinks.k2.hdfs.inUsePrefix = _

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Use a channel which buffers events in memory
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c1









