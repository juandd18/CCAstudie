
//program spark streaming

object StreamingRequestCount {

  def main(args: Array[String]) {
  
  val sc = new SparkContext()
  val ssc = new StreamingContext(sc,Seconds(2))
  val mystream = ssc.socketTextStream(hostname, port)
  val userreqs = mystream
  .map(line => (line.split(' ')(2),1))
  .reduceByKey((x,y) => x+y)
  
  val sortedreqs = userreqs
  .map(pair => pair.swap)
  .transform(rdd => rdd.sortByKey(false))
  
  sortedreqs.foreachRDD((rdd,time) => {
        println("Top users @ " + time)
        rdd.take(5).foreach(
        pair => printf("User: %s (%s)\n",pair._2, pair._1))
      }
  )
  
  
  userreqs.print()
  
  ssc.start()
  ssc.awaitTermination()
  }
}

//StreamingContext(sc,Seconds(2))
//Configured with the same parameters as a SparkContext
//plus batch duration—instance of Milliseconds, Seconds, or Minutes

transform(function)
– Creates a new DStream by executing function on RDDs in the
current DStream

//taller

import org.apache.spark.SparkContext
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds

object StreamingLogs {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: solution.StreamingLogs <hostname> <port>")
      System.exit(1)
    } 
 
    // get hostname and port of data source from application arguments
    val hostname = args(0)
    val port = args(1).toInt
    
    // Create a new SparkContext
    val sc = new SparkContext()

    // Set log level to ERROR to avoid distracting extra output
    sc.setLogLevel("ERROR")

    // Create and configure a new Streaming Context 
    // with a 1 second batch duration
    val ssc = new StreamingContext(sc,Seconds(1))

    // Create a DStream of log data from the server and port specified   
    val logs = ssc.socketTextStream(hostname,port)

    // Filter the DStream to only include lines containing the string “KBDOC”
    val kbreqs = logs.filter(line => line.contains("KBDOC"))

    // Test application by printing out the first 5 lines received in each batch 
    kbreqs.print(5)

    // Save the filtered logs to text files
    kbreqs.saveAsTextFiles("/loudacre/streamlog/kblogs")

    // Print out the count of each batch RDD in the stream
    kbreqs.foreachRDD(rdd => println("Number of KB requests: " + rdd.count()))

    // Start the streaming context and then wait for application to terminate
    ssc.start()
    ssc.awaitTermination()

  }
}

//kafka

import org.apache.spark.SparkContext
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming.kafka._
import kafka.serializer.StringDecoder

object StreamingLogsKafka {
  def main(args: Array[String]) {
    if (args.length < 1) {
      System.err.println("Usage: solution.StreamingLogsKafka <topic>")
      System.exit(1)
    }  
    val topic = args(0)
    
    val sc = new SparkContext()
    sc.setLogLevel("ERROR")

    // Configure the Streaming Context with a 1 second batch duration
    val ssc = new StreamingContext(sc,Seconds(1))

    // Create a DStream of log data from Kafka topic  
    val kafkaStream = KafkaUtils.
       createDirectStream[String,String,StringDecoder,StringDecoder](ssc, 
       Map("metadata.broker.list"->"localhost:9092"), 
       Set(topic))

    // The weblog data is in the form (key, value), map to just the value
    val logs = kafkaStream.map(pair => pair._2)

    // To test, print the first few lines of each batch of messages to confirm receipt
    logs.print()
        
    // Print out the count of each batch RDD in the stream
    logs.foreachRDD(rdd => println("Number of requests: " + rdd.count()))


    // Save the logs
    logs.saveAsTextFiles("/loudacre/streamlog/kafkalogs")
    
    ssc.start()
    ssc.awaitTermination()
  }
}



