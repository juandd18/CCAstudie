//Ejercicio 48

{
  "type" : "record",
  "name" : "sqoop_import_departments",
  "doc" : "Sqoop import of departments",
  "fields" : [ {
    "name" : "department_id",
    "type" : [ "int", "null" ],
    "columnName" : "department_id",
    "sqlType" : "4"
  }, {
    "name" : "department_name",
    "type" : [ "string", "null" ],
    "columnName" : "department_name",
    "sqlType" : "12"
  } ],
  "tableName" : "departments"
}

1.create a hive table avro with the schema above and import data using sqoop

//Ejercicio 79

1.creates tables in hive with different compression codecs

//Ejercicio 50

{
  "type" : "record",
  "name" : "orders_partition",
  "doc" : "Hive partitioned table schema ",
  "fields" : [ {
    "name" : "order_id",
    "type" : [ "int", "null" ]
  }, {
    "name" : "order_date",
    "type" : [ "long", "null" ]
  }, {
    "name" : "order_customer_id",
    "type" : [ "int", "null" ]
  }, {
    "name" : "order_status",
    "type" : [ "string", "null" ]
  } ],
  "tableName" : "orders_partition"
}

1.create a manage table from orders which is partitioned (order_month) using avro above
2.create a manual partition 2014-02
3.add data dynamically to the partition table

//Ejercicio 52

first_name,last_name,address,country,city,state,pincode,home,office,email,website

1.create a table partition by state and country
2.add data to the partition state=rj country=in
3.create another table by state and country and insert dynamically with
the first table

//Ejercicio 53

{
  "type" : "record",
  "name" : "orders_partition4",
  "doc" : "Hive partitioned table schema ",
  "fields" : [ {
    "name" : "order_id",
    "type" : [ "int", "null" ]
  }, {
    "name" : "order_date",
    "type" : [ "long", "null" ]
  }, {
    "name" : "order_customer_id",
    "type" : [ "int", "null" ]
  }, {
    "name" : "order_status",
    "type" : [ "string", "null" ]
  } ],
  "tableName" : "orders_partition4"
}

1.create a hive table with the above schema with two
columns order_value default value -99999, order_description default value
"not defined" not null  in both new fields

2. load data from orders


//Ejercicio 54

1.using sqoop import join between categories and departments base
on department_id

//Ejercicio 55

Purchase.txt
1|Amit|Jain|Soap|299
2|Ankur|Sharma|Detergent|480
3|Avinash|Saha|Shampoo|560
4|Anirudh|Makan|Brush|700
5|Avinash|Temkar|Cloths|1060

Wallet.txt
1|799
2|480
3|790
4|1000
5|1200

1.find a wallet balance less or equal to 500 (using spark and pig)

//Ejercicio 56

Name|SName|item|Price
Amit|Jain|Soap|299
Ankur|Sharma|Detergent|480
Avinash|Saha|Shampoo|560
Anirudh|Makan|Brush|700
Avinash|Temkar|Cloths|1060
Amit|Jain|Soap|399
Ankur|Sharma|Detergent|560
Avinash|Saha|Shampoo|730
Anirudh|Makan|Brush|820
Avinash|Temkar|Cloths|444
Amit|Jain|Soap|555
Ankur|Sharma|Detergent|666
Avinash|Saha|Shampoo|777
Anirudh|Makan|Brush|899
Avinash|Temkar|Cloths|55

1.remove duplicates from above fields base on name,sname and item

//Ejercicio 57

REPRESEN.csv
repid,repname,territoryid
1,juan,2
2,maria,1
3,vamilo,2
4,dario,1

purchases.csv
id,orderid,salesamount
4,2,100
2,2,200
2,3,600
3,4,80
4,5,120
1,6,170
3,7,140

create hive tables and load those tables
join both file by the first column (repid) and order by all sales representers order by salesamount and their position
join both file by the first column (repid) and  order the sales representers base on their sale within a territory

//Ejercicio 58

1. create a table in retail_db
departments_new(
department_id int(11),
department_name varchar(45),
created_date timestamp default now()
)
2. insert data from departments to departments_new

3.write a oozie which import data from departments_new to hdfs /user/ccp/oozie/problem58/data

//Ejercicio 59

1. create a table in retail_db
departments_exported(
department_id int(11),
department_name varchar(45),
created_date timestamp default now()
)

2. export data from hdfs (/user/ccp/oozie/problem58/data) to departments_exported(

//Ejercicio 60

name,host,product,visit
juan,cloudera.com,hive,23
felipoe,facebook.com,impala,12
maria,linkedin.com,hbase,7
lucas,juna.com,link,11

1. create a pig program which have visit more than 15 and order by visit
2. create a oozie workflow to run the pig program.

//Ejercicio 61

1. create a oozie workflow
  1.1 delete a destination folder
  1.2 create a folder ("destination folder")
  1.3 change permisson of the folder ()

//Ejercicio 62

Jun  3 07:10:54 hadoopexam.com_node01 init: tty (/dev/tty6) main process (1208) killed by TERM signal
Jun  3 07:11:31 hadoopexam.com_node01 kernel: registered taskstats version 1
Jun  3 07:11:31 hadoopexam.com_node01 kernel: sr0: scsi3-mmc drive: 32x/32x xa/form2 tray
Jun  3 07:11:31 hadoopexam.com_node01 kernel: piix4_smbus 0000:00:07.0: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
Jun  3 07:11:31 hadoopexam.com_node01 kernel: nf_conntrack version 0.5.0 (7972 buckets, 31888 max)
Jun  3 07:11:57 hadoopexam.com_node01 kernel: hrtimer: interrupt took 11250457 ns
Jun  3 07:11:59 hadoopexam.com_node01 ntpd_initres[1705]: host name not found: 0.rhel.pool.ntp.org

1.0 create a table in mysql with month,day,date,node,process, message
1. create a pig program which read the file above and store it in hdfs
2. create a oozie workflow which verify that the file above has data
  2.2 then run the pig program
  2.3 call another workflow which should export the ouput of the pig progran
      to the mysql

//Ejercicio 63

Jun  3 07:10:54 hadoopexam.com_node01 init: tty (/dev/tty6) main process (1208) killed by TERM signal
Jun  3 07:11:31 hadoopexam.com_node01 kernel: registered taskstats version 1
Jun  3 07:11:31 hadoopexam.com_node01 kernel: sr0: scsi3-mmc drive: 32x/32x xa/form2 tray
Jun  3 07:11:31 hadoopexam.com_node01 kernel: piix4_smbus 0000:00:07.0: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
Jun  3 07:11:31 hadoopexam.com_node01 kernel: nf_conntrack version 0.5.0 (7972 buckets, 31888 max)
Jun  3 07:11:57 hadoopexam.com_node01 kernel: hrtimer: interrupt took 11250457 ns
Jun  3 07:11:59 hadoopexam.com_node01 ntpd_initres[1705]: host name not found: 0.rhel.pool.ntp.org
July  3 07:10:54 hadoopexam.com_node01 init: tty (/dev/tty6) main process (1208) killed by TERM signal
July  3 07:11:31 hadoopexam.com_node01 kernel: registered taskstats version 1
July  3 07:11:31 hadoopexam.com_node01 kernel: sr0: scsi3-mmc drive: 32x/32x xa/form2 tray
July  3 07:11:31 hadoopexam.com_node01 kernel: piix4_smbus 0000:00:07.0: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
July  3 07:11:31 hadoopexam.com_node01 kernel: nf_conntrack version 0.5.0 (7972 buckets, 31888 max)
July  3 07:11:57 hadoopexam.com_node01 kernel: hrtimer: interrupt took 11250457 ns
July  3 07:11:59 hadoopexam.com_node01 ntpd_initres[1705]: host name not found: 0.rhel.pool.ntp.org

1.0 create a table in mysql with month,day,date,node,process, message
1. create a pig program which read the file above and store it in hdfs
2. create a oozie workflow which verify that the file above has data
  2.2 then run the pig program
  2.3 call another workflow which should export the ouput of the pig progran
      to the mysql
  2.4 create another flow to import that data (from the mysql table just created )
      to three different hive tables

//Ejercicio 64

1. create a oozie workflow to use flume spooldir to load data from local to hdfs

//Ejercicio 65

1. create a oozie workflow to import all tables of mysql to hive partition with 3 files

//Ejercicio 66

1. create a oozie workflow to import three tables using fork and join

//Ejercicio 67

1. create a oozie workflow to import categories tables where category_id is between 1 and 10

//Ejercicio 68

1. create a oozie workflow
2. create a folder, two files
3. move those file to another folder
4. if is okey send a email

//Ejercicio 69

using oozie
import a table changing the fields delimiter and lines delimiter

//Ejercicio 70

hadoopexam_jun16.log
Jun  3 07:10:54 hadoopexam.com_node01 init: tty (/dev/tty6) main process (1208) killed by TERM signal
Jun  3 07:11:31 hadoopexam.com_node01 kernel: registered taskstats version 1
Jun  3 07:11:31 hadoopexam.com_node01 kernel: sr0: scsi3-mmc drive: 32x/32x xa/form2 tray
Jun  3 07:11:31 hadoopexam.com_node01 kernel: piix4_smbus 0000:00:07.0: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
Jun  3 07:11:31 hadoopexam.com_node01 kernel: nf_conntrack version 0.5.0 (7972 buckets, 31888 max)
Jun  3 07:11:57 hadoopexam.com_node01 kernel: hrtimer: interrupt took 11250457 ns
Jun  3 07:11:59 hadoopexam.com_node01 ntpd_initres[1705]: host name not found: 0.rhel.pool.ntp.org

hadoopexam_july16.log
July  3 07:10:54 hadoopexam.com_node01 init: tty (/dev/tty6) main process (1208) killed by TERM signal
July  3 07:11:31 hadoopexam.com_node01 kernel: registered taskstats version 1
July  3 07:11:31 hadoopexam.com_node01 kernel: sr0: scsi3-mmc drive: 32x/32x xa/form2 tray
July  3 07:11:31 hadoopexam.com_node01 kernel: piix4_smbus 0000:00:07.0: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
July  3 07:11:31 hadoopexam.com_node01 kernel: nf_conntrack version 0.5.0 (7972 buckets, 31888 max)
July  3 07:11:57 hadoopexam.com_node01 kernel: hrtimer: interrupt took 11250457 ns
July  3 07:11:59 hadoopexam.com_node01 ntpd_initres[1705]: host name not found: 0.rhel.pool.ntp.org


1. create a external hive table which load the files above using regex,
2. creates a oozie workflow (hive action) which creates a new table

//Ejercicio 71
Revisar solucion

//Ejercicio 72
1. launch a schedule taks to run daily each five minutes from one date to another.
//Ejercicio 73
1. launch a schedule taks to run daily from one date to another.

//Ejercicio 79

1.creates tables in hive with different compression codecs
